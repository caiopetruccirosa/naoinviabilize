{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# NOTE: This is ONLY necessary in jupyter notebook.\n",
    "# Details: Jupyter runs an event-loop behind the scenes.\n",
    "#          This results in nested event-loops when we start an event-loop to make async queries.\n",
    "#          This is normally not allowed, we use nest_asyncio to allow it for convenience.\n",
    "import nest_asyncio\n",
    "\n",
    "\n",
    "from llama_index.core import Document, QueryBundle\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.llms.groq import Groq\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.tools import tool\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "import spacy\n",
    "\n",
    "from glob import glob\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "from thefuzz import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().handlers = []\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_WINDOW_SIZE = 5\n",
    "SENTENCE_WINDOW_STRIDE = 2\n",
    "\n",
    "TOP_K = 4\n",
    "BM25_TOP_K = 1000\n",
    "\n",
    "LLM_MODEL = \"llama3-70b-8192\"\n",
    "LLM_TEMPERATURE = 0\n",
    "RERANKER_MODEL = \"unicamp-dl/monoptt5-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcription_documents(base_transcriptions_path):\n",
    "    transcription_documents = []\n",
    "\n",
    "    for transcriptions_path in glob(base_transcriptions_path):\n",
    "        with open(transcriptions_path) as f:\n",
    "            transcriptions = json.load(f)\n",
    "        \n",
    "        for transcription in transcriptions:\n",
    "            transcription_documents.append(\n",
    "                Document(\n",
    "                    text=transcription['transcription'],\n",
    "                    metadata={\n",
    "                        'title': transcription['title'],\n",
    "                        'publishing_date': transcription['publishing_date'],\n",
    "                        'quadro': transcription['quadro'],\n",
    "                        'hashtag': transcription['hashtag'],\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "            \n",
    "    return transcription_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_split(documents, stride, window_size):\n",
    "    sentencizer = spacy.blank('pt')\n",
    "    sentencizer.add_pipe('sentencizer')\n",
    "    \n",
    "    window_documents = []\n",
    "\n",
    "    for document in documents:\n",
    "        doc_sentencized = sentencizer(document.text)\n",
    "        sentences = [sent.text.strip() for sent in doc_sentencized.sents]\n",
    "        for i in range(0, len(sentences), stride):\n",
    "            window_text = ' '.join(sentences[i : min(len(sentences), i+window_size)]).strip()\n",
    "            window_metadata = document.metadata.copy()\n",
    "            window_metadata['parent_document_id'] = document.id_\n",
    "            window_documents.append(Document(text=window_text, metadata=window_metadata))\n",
    "\n",
    "    return window_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcriptions_nodes(transcription_documents):\n",
    "    transcription_window_documents = sliding_window_split(transcription_documents, SENTENCE_WINDOW_STRIDE, SENTENCE_WINDOW_SIZE)\n",
    "    \n",
    "    transcription_nodes = dict()\n",
    "    for document in transcription_window_documents:\n",
    "        new_node = TextNode(id=document.id_, text=document.text, metadata=document.metadata)\n",
    "        if new_node.metadata['title'] not in transcription_nodes.keys():\n",
    "            transcription_nodes[document.metadata['title']] = [new_node]\n",
    "        else:\n",
    "            transcription_nodes[document.metadata['title']].append(new_node)\n",
    "    \n",
    "    return transcription_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription_documents = get_transcription_documents(\"../transcriptions-headless/*.json\")\n",
    "\n",
    "transcription_documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription_nodes = get_transcriptions_nodes(transcription_documents)\n",
    "\n",
    "transcription_nodes['milton']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RAGResponse:\n",
    "    answer: str\n",
    "    contexts: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiIndexRetriever:\n",
    "    def __init__(self, indexes_nodes: Dict[str, List[TextNode]], top_k, bm25_top_k, reranker_model) -> None:\n",
    "        self.indexes: Dict[str, List[TextNode]] = indexes_nodes\n",
    "        \n",
    "        self.bm25_retrievers: Dict[str, BM25Retriever] = dict()\n",
    "        for index, nodes in indexes_nodes.items():\n",
    "            self.bm25_retrievers[index] = BM25Retriever.from_defaults(nodes=nodes, similarity_top_k=bm25_top_k)\n",
    "\n",
    "        self.reranker = SentenceTransformerRerank(top_n=top_k, model=reranker_model)\n",
    "        \n",
    "\n",
    "    def retrieve(self, index_name: str, query: str) -> List[str]:\n",
    "        if index_name not in self.indexes:\n",
    "            raise ValueError(f\"Index {index_name} not found\")\n",
    "        \n",
    "        retriever = self.bm25_retrievers[index_name]\n",
    "\n",
    "        retrieved_nodes = retriever.retrieve(query)\n",
    "        reranked_nodes = self.reranker.postprocess_nodes(\n",
    "            retrieved_nodes,\n",
    "            query_bundle=QueryBundle(query),\n",
    "        )\n",
    "\n",
    "        context_chunks = [ node.get_text() for node in reranked_nodes ]\n",
    "\n",
    "        return context_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexDetector:\n",
    "    def __init__(self, llm_model: str, index_names: List[str], GROQ_API_KEY: str = os.environ['GROQ_API_KEY']):\n",
    "        self._base_prompt = \\\n",
    "            \"Given the following Portuguese query, regarding an episode of a podcast, please tell me the title of the episode. \" \\\n",
    "            \"The query starts now: '{query}'.\" \\\n",
    "            \"You MUST answer with only the title of the episode.\"\n",
    "        \n",
    "        self.llm = Groq(model=llm_model, api_key=GROQ_API_KEY, temperature=LLM_TEMPERATURE)\n",
    "        self.llm_model_name = llm_model\n",
    "        self.index_names = index_names\n",
    "\n",
    "    def detect_index(self, query: str) -> str:\n",
    "        prompt = self._base_prompt.format(query=query)\n",
    "        raw_llm_guess = self.llm.complete(prompt).text\n",
    "        llm_guess = raw_llm_guess.strip().lower()\n",
    "\n",
    "        if llm_guess not in self.index_names:\n",
    "            index_name = llm_guess\n",
    "        else:\n",
    "            index_name, _ = process.extractOne(llm_guess, self.index_names)\n",
    "            \n",
    "        return index_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGGenerator:\n",
    "    def __init__(self, llm_model, GROQ_API_KEY=os.environ['GROQ_API_KEY']):\n",
    "        self._base_prompt = \\\n",
    "            \"Consider the following context passages of a podcast episode and answer the given question.\" \\\n",
    "            \"You MUST answer the question only in Portuguese.\" \\\n",
    "            \"\\n\\n\" \\\n",
    "            \"{context_passages}\" \\\n",
    "            \"\\n\\n\" \\\n",
    "            \"If there is not enough information in the context passages, answer \\\"Não há informação suficiente no episódio.\\\".\" \\\n",
    "            \"\\n\\n\" \\\n",
    "            \"Question: {query}\"\n",
    "        \n",
    "        self.llm = Groq(model=llm_model, api_key=GROQ_API_KEY, temperature=LLM_TEMPERATURE)\n",
    "        self.llm_model_name = llm_model\n",
    "\n",
    "    def generate_answer(self, query: str, contexts: List[str]) -> str:\n",
    "        context_passages = \"\\n\\n\".join([ f\"Context {i}: {context}\" for i, context in enumerate(contexts, 1) ])\n",
    "        prompt = self._base_prompt.format(query=query, context_passages=context_passages)\n",
    "        answer = self.llm.complete(prompt).text.strip()\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGPipeline:\n",
    "    def __init__(\n",
    "        self,\n",
    "        retriever: MultiIndexRetriever, \n",
    "        index_detector: IndexDetector,\n",
    "        generator: RAGGenerator,\n",
    "    ) -> None:\n",
    "        self.index_detector: IndexDetector = index_detector\n",
    "        self.retriever: MultiIndexRetriever = retriever\n",
    "        self.generator: RAGGenerator = generator\n",
    "\n",
    "    def __call__(self, query: str) -> RAGResponse:\n",
    "        index_name = self.index_detector.detect_index(query)\n",
    "        context_chunks = self.retriever.retrieve(index_name, query)\n",
    "        answer = self.generator.generate_answer(query, context_chunks)\n",
    "        return RAGResponse(answer, context_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_detector = IndexDetector(llm_model=LLM_MODEL, index_names=list(transcription_nodes.keys()))\n",
    "index_name = index_detector.detect_index('No episódio \\'mário\\', quem foi diagnosticado com câncer de bexiga na história?')\n",
    "index_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_contexts = [\n",
    "    \"Eu sou Bia Suzuki, tenho 30 anos, moro em São José do Rio Preto, interior de São Paulo... eu recebi o diagnóstico de câncer de intestino aos 25 anos...\",\n",
    "    \"O câncer colorretal é a terceira neoplasia mais frequente e a segunda de maior mortalidade no mundo...\",\n",
    "    \"Eu achei que não é impossível viver bem com bolsinha e eu sou uma prova disso.\"\n",
    "]\n",
    "example_question = \"No episódio 'bia', quem é a Bia e o que aconteceu com ela?\"\n",
    "\n",
    "rag_generator = RAGGenerator(llm_model=LLM_MODEL)\n",
    "rag_generator.generate_answer(example_question, example_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_index_retriever = MultiIndexRetriever(\n",
    "    indexes_nodes=transcription_nodes,\n",
    "    top_k=TOP_K,\n",
    "    bm25_top_k=BM25_TOP_K,\n",
    "    reranker_model=RERANKER_MODEL\n",
    ")\n",
    "index_detector = IndexDetector(llm_model=LLM_MODEL, index_names=list(transcription_nodes.keys()))\n",
    "rag_generator = RAGGenerator(llm_model=LLM_MODEL)\n",
    "\n",
    "rag_pipeline = RAGPipeline(\n",
    "    retriever=multi_index_retriever,\n",
    "    index_detector=index_detector,\n",
    "    generator=rag_generator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt adaptado a partir da combinação dos seguintes prompts:\n",
    "#   - https://github.com/run-llama/llama_index/blob/a87b63fce3cc3d24dc71ae170a8d431440025565/llama_index/agent/react/prompts.py\n",
    "#   - https://smith.langchain.com/hub/hwchase17/react-chat\n",
    "\n",
    "REACT_CHAT_SYSTEM_HEADER = \"\"\"\\\n",
    "You are designed to help answering questions regarding the content of the Não Inviabilize podcast episodes.\n",
    "You are a Large Language Model trained by Meta AI. As a language model, you are able to generate human-like text based on the input you receive, allowing yourself to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
    "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions.\n",
    "Additionally, you are able to generate your own text based on the input it receives, allowing yourself to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
    "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. \n",
    "When the user needs help with a specific question about something that was discussed on an specific episode of the Não Inviabilize podcast, you are here to assist.\n",
    "\n",
    "TOOLS:\n",
    "------\n",
    "You have access to a variety of tools that can help you get information to answer questions.\n",
    "You are responsible for using the tools in any sequence you deem appropriate to complete the task at hand.\n",
    "This may require breaking the task into subtasks and using different tools to complete each subtask.\n",
    "You have access to the following tools:\n",
    "{tools}\n",
    "\n",
    "To use a tool, please use the following format:\n",
    "```\n",
    "Thought: Do I need to use a tool? Yes\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "```\n",
    "\n",
    "When you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n",
    "```\n",
    "Thought: Do I need to use a tool? No\n",
    "Final Answer: [your response here]\n",
    "```\n",
    "But be careful, you MUST answer the question in Portuguese only!\n",
    "\n",
    "If there is not enough information in the context passages, answer \"Não há informação suficiente no episódio.\"\n",
    "\n",
    "Begin!\n",
    "User question: {input}\n",
    "\n",
    "{agent_scratchpad}\n",
    "\"\"\"\n",
    "\n",
    "class ReActRAGPipeline:\n",
    "    def __init__(\n",
    "        self,\n",
    "        retriever: MultiIndexRetriever, \n",
    "        index_detector: IndexDetector,\n",
    "        verbose: bool = False,\n",
    "    ) -> None:\n",
    "        self.index_detector: IndexDetector = index_detector\n",
    "        self.retriever: MultiIndexRetriever = retriever\n",
    "\n",
    "        base_prompt = PromptTemplate.from_template(REACT_CHAT_SYSTEM_HEADER)\n",
    "        llm = ChatGroq(\n",
    "            temperature=LLM_TEMPERATURE,\n",
    "            model_name=LLM_MODEL,\n",
    "            api_key=os.environ['GROQ_API_KEY']\n",
    "        )\n",
    "\n",
    "        tools = [self._search_episode_title, self._search_relevant_passages]\n",
    "        agent = create_react_agent(llm, tools, base_prompt)\n",
    "        agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=verbose, handle_parsing_errors=True)\n",
    "\n",
    "        self._search_history = []\n",
    "\n",
    "        self.agent_executor = agent_executor\n",
    "\n",
    "    @tool\n",
    "    def _search_relevant_passages(self, episode_title: str, question: str) -> list[str]:\n",
    "        \"\"\"\n",
    "        Returns a sequence of relevant contexts passages for the given question about a specific episode in the podcast.\n",
    "        In order to perform this search, you must provide the question in Portuguese and the title of the episode.\n",
    "        \"\"\"\n",
    "        contexts = self.retriever.retrieve(episode_title, question)\n",
    "        self._search_history += contexts\n",
    "        return contexts\n",
    "\n",
    "\n",
    "    @tool\n",
    "    def _search_episode_title(self, question: str) -> list[str]:\n",
    "        \"\"\"\n",
    "        Returns the title of the episode that is being asked about in the given question.\n",
    "        In order to perform this search, you must provide the question in Portuguese.\n",
    "        \"\"\"\n",
    "        return self.index_detector.detect_index(question)\n",
    "    \n",
    "    def generate_answer(self, question: str) -> RAGResponse:\n",
    "        self._search_history = []\n",
    "        output = self.agent_executor.invoke({\"input\": \"No episódio 'bia', quem é a Bia e o que aconteceu com ela?\"})\n",
    "        \n",
    "        answer = output['output']\n",
    "        contexts = self._search_history\n",
    "        \n",
    "        self._search_history = []\n",
    "\n",
    "        return RAGResponse(answer, contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "react_rag_pipeline = ReActRAGPipeline(\n",
    "    retriever=multi_index_retriever, \n",
    "    index_detector=index_detector,\n",
    ")\n",
    "react_rag_pipeline.generate_answer(\"No episódio 'bia', quem é a Bia e o que aconteceu com ela?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "naoinviabilize",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
