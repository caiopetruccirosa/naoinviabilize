{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mZGoPbxRbDC"
      },
      "source": [
        "# Bibliotecas e pacotes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xw5LYxRY3_ch",
        "outputId": "02951dca-116b-4e2f-c478-6318d3722da4"
      },
      "outputs": [],
      "source": [
        "!apt install openjdk-21-jdk-headless"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7ak2eu54Luh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-21-openjdk-amd64'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrhLbP9qe1v-",
        "outputId": "1ad8729e-5109-434c-d725-2e30ee705521"
      },
      "outputs": [],
      "source": [
        "!pip install pyserini faiss-cpu\n",
        "!pip install -q groq\n",
        "!pip install -q beautifulsoup4\n",
        "!pip install langchain\n",
        "!pip install langchain-groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DI73RXJHewZM"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata, drive\n",
        "from groq import Groq, RateLimitError\n",
        "from tqdm import tqdm\n",
        "from bs4 import BeautifulSoup\n",
        "from pyserini.search.lucene import LuceneSearcher\n",
        "from collections import Counter\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.tools import tool\n",
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "import json\n",
        "import threading\n",
        "import time\n",
        "import json\n",
        "import spacy\n",
        "import argparse\n",
        "import collections\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import sys\n",
        "import unicodedata\n",
        "import pandas as pd\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeHiF2qqfZSp"
      },
      "source": [
        "# Atributos e hiper-parâmetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKNiJRASfbhK"
      },
      "outputs": [],
      "source": [
        "LLM_MODEL_NAME = \"llama3-70b-8192\"\n",
        "LLM_TEMPERATURE = 0\n",
        "\n",
        "DOCUMENT_WINDOW_STRIDE = 3\n",
        "DOCUMENT_WINDOW_SIZE = 2\n",
        "\n",
        "RETRIEVER_TOP_K = 5\n",
        "\n",
        "N_QUESTIONS = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75fqOrQcHRJm"
      },
      "outputs": [],
      "source": [
        "GROQ_API_KEY = userdata.get('GROQ_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLS-hNMRyrUq"
      },
      "outputs": [],
      "source": [
        "# Prompt adaptado a partir da combinação dos seguintes prompts:\n",
        "#   - https://github.com/run-llama/llama_index/blob/a87b63fce3cc3d24dc71ae170a8d431440025565/llama_index/agent/react/prompts.py\n",
        "#   - https://smith.langchain.com/hub/hwchase17/react-chat (entrei em contato através do código do Fábio Grassiotto)\n",
        "\n",
        "REACT_CHAT_SYSTEM_HEADER = \"\"\"\\\n",
        "\n",
        "You are designed to help with a variety of tasks, from answering questions to providing summaries to other types of analyses.\n",
        "You are a Large Language Model trained by Meta AI.\n",
        "As a language model, you are able to generate human-like text based on the input you receive, allowing yourself to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
        "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions.\n",
        "Additionally, you are able to generate your own text based on the input it receives, allowing yourself to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
        "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the user needs help with a specific question or just want to have a conversation about a particular topic, you are here to assist.\n",
        "\n",
        "TOOLS:\n",
        "------\n",
        "You have access to a wide variety of tools.\n",
        "You are responsible for using the tools in any sequence you deem appropriate to complete the task at hand.\n",
        "This may require breaking the task into subtasks and using different tools to complete each subtask.\n",
        "You have access to the following tools:\n",
        "{tools}\n",
        "\n",
        "To use a tool, please use the following format:\n",
        "```\n",
        "Thought: Do I need to use a tool? Yes\n",
        "Action: the action to take, should be one of [{tool_names}]\n",
        "Action Input: the input to the action\n",
        "Observation: the result of the action\n",
        "```\n",
        "\n",
        "When you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n",
        "```\n",
        "Thought: Do I need to use a tool? No\n",
        "Final Answer: [your response here]\n",
        "```\n",
        "\n",
        "Your final answer must be short, no more than 10 words, and use numerals instead of words for numbers.\n",
        "If you don't know any plausible answer, answer \"Not enough information provided in the documents.\"\n",
        "\n",
        "Begin!\n",
        "New input: {input}\n",
        "{agent_scratchpad}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Giyi5Rv_NIm"
      },
      "source": [
        "# Dataset Não Inviabilize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYLi95LnZ7kg"
      },
      "outputs": [],
      "source": [
        "# load dataset transcriptions\n",
        "# load dataset questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAYAeAUaCgmh"
      },
      "source": [
        "# Indexação e pré-processamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VnAWNmqNnu8"
      },
      "outputs": [],
      "source": [
        "# preprocess (make chunks, ...) and index transcriptions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ma0HBKrx5Nis"
      },
      "outputs": [],
      "source": [
        "def remove_html_tags(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "def extract_data(num_questions, dataset, context_articles):\n",
        "    questions_found = []\n",
        "    num_questions_found = 0\n",
        "    documents = []\n",
        "    all_titles = []\n",
        "\n",
        "    for item in tqdm(dataset):\n",
        "        question = item['question']\n",
        "        answer = item['answer']\n",
        "        answer_type = answer['type']\n",
        "\n",
        "        if answer_type == 'binary' or answer_type == 'value':\n",
        "            final_answer = answer['answer_value']\n",
        "        elif answer_type == 'span':\n",
        "            final_answer = answer['answer_spans'][0]['text']\n",
        "        elif answer_type == 'none':\n",
        "            final_answer = 'none'\n",
        "        else:\n",
        "            final_answer = 'An error perhaps, bad type'\n",
        "\n",
        "        if final_answer == 'none':\n",
        "            # Skip this one.\n",
        "            continue\n",
        "        else:\n",
        "            # Thats a good question.\n",
        "\n",
        "            # First add some extra info in the context-part\n",
        "            for context in item['context']:\n",
        "                if context['passage'] == \"main\":\n",
        "                    # Cleanup html tags\n",
        "                    clean_text = remove_html_tags(context['text'])\n",
        "                    documents.append({\n",
        "                        \"title\": item['title'].lower(),\n",
        "                        \"content\": clean_text\n",
        "                    })\n",
        "\n",
        "            all_titles.append(item['title'].lower())\n",
        "\n",
        "            # And then grab the text from the articles json\n",
        "            for link in item[\"question_links\"]:\n",
        "                if link.lower() in context_articles and link.lower() not in all_titles:\n",
        "                  # Cleanup html tags\n",
        "                  soup = BeautifulSoup(context_articles[link.lower()], 'html.parser')\n",
        "                  clean_text = soup.get_text()\n",
        "\n",
        "                  documents.append({\n",
        "                    \"title\": link,\n",
        "                    \"content\": clean_text\n",
        "                  })\n",
        "                all_titles.append(link.lower())\n",
        "\n",
        "            questions_found.append({\"Question\": question, \"Answer\": final_answer})\n",
        "            num_questions_found += 1\n",
        "\n",
        "            if num_questions_found == num_questions:\n",
        "                # found our questions\n",
        "                break\n",
        "\n",
        "    return questions_found, documents, all_titles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITmxX84xeFCF",
        "outputId": "e38c3a03-939a-4575-abcb-d4f0a51983af"
      },
      "outputs": [],
      "source": [
        "questions_to_ask, documents, all_titles = extract_data(N_QUESTIONS, test_questions, context_articles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RokCO5djQvD",
        "outputId": "2541eecd-96c5-473a-fdb8-5053d21cac29"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.blank(\"en\")\n",
        "nlp.add_pipe(\"sentencizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lb4DpLU85PDH"
      },
      "outputs": [],
      "source": [
        "def sliding_window_split(documents, stride, window_size):\n",
        "    treated_documents = []\n",
        "\n",
        "    for j, document in enumerate(tqdm(documents)):\n",
        "        doc_text = document['content']\n",
        "        doc = nlp(doc_text[:10000])\n",
        "        sentences = [sent.text.strip() for sent in doc.sents]\n",
        "        for i in range(0, len(sentences), stride):\n",
        "            segment = ' '.join(sentences[i:i+window_size]).strip()\n",
        "            treated_documents.append({\n",
        "                \"title\": document['title'],\n",
        "                \"contents\": document['title']+ \". \" + segment,\n",
        "                \"segment\": segment\n",
        "            })\n",
        "            if i+window_size >= len(sentences):\n",
        "                break\n",
        "\n",
        "    return treated_documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcQ2A4umkMuI"
      },
      "outputs": [],
      "source": [
        "def add_id_and_filter_empty(documents):\n",
        "    filtered_documents = []\n",
        "    for i, doc in enumerate(documents):\n",
        "        if doc['segment'] != \"\":\n",
        "            filtered_doc = { **doc }\n",
        "            filtered_doc['id'] = i\n",
        "            filtered_documents.append(filtered_doc)\n",
        "    return filtered_documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdkqENrskJy5",
        "outputId": "266704bf-5bb1-4c6a-d33b-be8b9c8a2f75"
      },
      "outputs": [],
      "source": [
        "treated_documents = add_id_and_filter_empty(sliding_window_split(documents, stride=DOCUMENT_WINDOW_STRIDE, window_size=DOCUMENT_WINDOW_SIZE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVakqEb3nGFY",
        "outputId": "e3f1e292-1f30-446a-d42e-c06038e54d5e"
      },
      "outputs": [],
      "source": [
        "!mkdir iirc_index_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLYm-JUclZl_"
      },
      "outputs": [],
      "source": [
        "with open(\"iirc_index_content/contents.jsonl\",'w') as file:\n",
        "    for doc in treated_documents:\n",
        "        file.write(json.dumps(doc)+\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YkqviJjaYSp",
        "outputId": "bc720e82-da6c-46c6-dc25-d45baed49743"
      },
      "outputs": [],
      "source": [
        "!python3 -m pyserini.index.lucene -collection JsonCollection -generator DefaultLuceneDocumentGenerator -threads 1 -input iirc_index_content -index iirc_index -storeRaw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYBi_rK_dfES"
      },
      "source": [
        "# Retriever com BM25 + MonoPTT5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pwiQdfIN8q7"
      },
      "outputs": [],
      "source": [
        "# use BM25 to retrieve something like 1000 documents\n",
        "# rerank with MonoPTT5 and get only top K (K could be 3 or 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtwcyWKoih1d"
      },
      "outputs": [],
      "source": [
        "class PyseriniRetriever:\n",
        "    def __init__(self, top_k):\n",
        "        self._searcher = LuceneSearcher('./iirc_index')\n",
        "        self._top_k = top_k\n",
        "\n",
        "    def __call__(self, query):\n",
        "        hits = self._searcher.search(query, k=self._top_k)\n",
        "        return [ json.loads(hit.lucene_document.get('raw')) for hit in hits ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puDHmhIgixX9"
      },
      "outputs": [],
      "source": [
        "retriever = PyseriniRetriever(RETRIEVER_TOP_K)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPXGetQKOUKP"
      },
      "source": [
        "# Abordagens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAzoCJw_OYHH"
      },
      "source": [
        "## Naive RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KYHT4PBk9Aj"
      },
      "source": [
        "## RAG baseado em ReAct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_RDtFTADQ4g"
      },
      "outputs": [],
      "source": [
        "@tool\n",
        "def get_topk_with_bm25(question: str) -> list[str]:\n",
        "    \"\"\"Returns a sequence of five document passages with texts to help solve a question.\"\"\"\n",
        "    searched_documents = retriever(question)\n",
        "    results = [ f\"Document passage {i}: {doc['contents']}\" for i, doc in enumerate(searched_documents, 1) ]\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOe-u_-eDK3r"
      },
      "outputs": [],
      "source": [
        "def get_agent_executor(verbose=False):\n",
        "    prompt = PromptTemplate.from_template(REACT_CHAT_SYSTEM_HEADER)\n",
        "    llm = ChatGroq(\n",
        "        temperature=LLM_TEMPERATURE,\n",
        "        model_name=LLM_MODEL_NAME,\n",
        "        api_key=GROQ_API_KEY\n",
        "    )\n",
        "\n",
        "    tools = [get_topk_with_bm25]\n",
        "    agent = create_react_agent(llm, [get_topk_with_bm25], prompt)\n",
        "    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=verbose, handle_parsing_errors=True)\n",
        "\n",
        "    return agent_executor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wk18fiAdJMc_"
      },
      "outputs": [],
      "source": [
        "agent_executor = get_agent_executor(verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTjGDtPtdhXN"
      },
      "source": [
        "# Avaliação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VgOxfynOmxk"
      },
      "source": [
        "## F1 Score e Exact Match"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ot2UXNhSsnxD"
      },
      "outputs": [],
      "source": [
        "def normalize_answer(s):\n",
        "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "\n",
        "    def remove_articles(text):\n",
        "        regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
        "        return re.sub(regex, ' ', text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    def remove_accents(input_str):\n",
        "        nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
        "        only_ascii = nfkd_form.encode('ASCII', 'ignore')\n",
        "        return only_ascii.decode(\"utf-8\")\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(remove_accents(s)))))\n",
        "\n",
        "\n",
        "def get_tokens(s):\n",
        "    if not s: return []\n",
        "    return normalize_answer(s).split()\n",
        "\n",
        "\n",
        "def compute_exact(a_gold, a_pred):\n",
        "    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
        "\n",
        "\n",
        "def compute_f1(a_gold, a_pred):\n",
        "    gold_toks = get_tokens(a_gold)\n",
        "    pred_toks = get_tokens(a_pred)\n",
        "    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
        "    num_same = sum(common.values())\n",
        "\n",
        "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
        "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
        "        return int(gold_toks == pred_toks)\n",
        "\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "\n",
        "    precision = 1.0 * num_same / len(pred_toks)\n",
        "    recall = 1.0 * num_same / len(gold_toks)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ix_S34KcF794"
      },
      "outputs": [],
      "source": [
        "def evaluate_agent_executor(executor):\n",
        "    df = pd.DataFrame(columns=['question', 'answer', 'LLM answer', 'F1', 'Exact Match'])\n",
        "\n",
        "    for item in tqdm(questions_to_ask):\n",
        "        question = item.get('Question')\n",
        "        answer = item.get('Answer')\n",
        "\n",
        "        agent_answer = executor.invoke({\"input\": \"Question: \" + question})\n",
        "        llm_answer = agent_answer['output']\n",
        "        f1_score = compute_f1(llm_answer, answer)\n",
        "        e_match_score = compute_exact(llm_answer, answer)\n",
        "\n",
        "        row = pd.Series(\n",
        "            [question, answer, llm_answer, f1_score, e_match_score],\n",
        "            index=df.columns\n",
        "        )\n",
        "        df = pd.concat(\n",
        "            [df, pd.DataFrame([row])],\n",
        "            ignore_index=True\n",
        "        )\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGbVZcIgGNz-",
        "outputId": "a75cee03-953e-4e94-92d3-ff0d6ba4a69b"
      },
      "outputs": [],
      "source": [
        "df = evaluate_agent_executor(agent_executor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "LxPOaXoKRCJ7",
        "outputId": "689134c1-97c3-4486-bcc8-ab636de29bc4"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxAiVS_otKQo",
        "outputId": "da1e8c47-eb5e-447d-9f11-26107f08fa79"
      },
      "outputs": [],
      "source": [
        "print(\"      Metrics     \")\n",
        "print(\"------------------\")\n",
        "print(f\"F1 score:\\n\\tAvg: {df['F1'].mean():.2f}.\\n\\tStd: {df['F1'].std():.2f}.\")\n",
        "print(f\"Exact Match score:\\n\\tAvg: {df['Exact Match'].mean():.2f}.\\n\\tStd: {df['Exact Match'].std():.2f}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e5U0D4FOrBN"
      },
      "source": [
        "## RAGAs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yqcp7bTZOsPJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "9mZGoPbxRbDC",
        "YeHiF2qqfZSp",
        "0Giyi5Rv_NIm",
        "iAYAeAUaCgmh",
        "XYBi_rK_dfES",
        "KPXGetQKOUKP",
        "7KYHT4PBk9Aj",
        "DTjGDtPtdhXN",
        "8VgOxfynOmxk"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
